FullyShardedDataParallel
========================

.. automodule:: torch.distributed.fsdp

.. autoclass:: torch.distributed.fsdp.FullyShardedDataParallel
  :members:

.. autoclass:: torch.distributed.fsdp.BackwardPrefetch
  :members:

.. autoclass:: torch.distributed.fsdp.ShardingStrategy
  :members:

.. autoclass:: torch.distributed.fsdp.MixedPrecision
  :members:

.. autoclass:: torch.distributed.fsdp.CPUOffload
  :members:

.. autoclass:: torch.distributed.fsdp.StateDictConfig
  :members:

.. autoclass:: torch.distributed.fsdp.StateDictConfig
  :members:

.. autoclass:: torch.distributed.fsdp.ShardedStateDictConfig
  :members:

.. autoclass:: torch.distributed.fsdp.StateDictConfig
  :members:

.. autoclass:: torch.distributed.fsdp.StateDictConfig
  :members:

.. autoclass:: torch.distributed.fsdp.StateDictConfig
  :members:

.. autoclass:: torch.distributed.fsdp.ShardedOptimStateDictConfig
  :members:

.. autoclass:: torch.distributed.fsdp.StateDictConfig
  :members:

.. autoclass:: torch.distributed.fsdp.StateDictSettings
  :members:
